{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN00NFS62IZaSS5Li8LyGhS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISL-0111/GDP_and_CO2/blob/main/Copy_of_Data_Engineering_Project_9_ISL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEET1J2OYSaU",
        "outputId": "dd422757-b928-4c4f-d702-ecae8c498599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Class : Natural Language Processing\n",
        "Assignment : Latent Dirichlet Allocation\n",
        "Name : Ilseop Lee\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# LDA Generator Function\n",
        "def lda_gen(\n",
        "    vocabulary: List[str], alpha: np.ndarray, beta: np.ndarray, xi: int\n",
        ") -> List[str]:\n",
        "    k = len(alpha)  # Number of topics\n",
        "    V = len(vocabulary)  # Vocabulary size\n",
        "    doc_length = np.random.poisson(xi)  # Poisson distribution with parameter xi\n",
        "    theta = np.random.dirichlet(alpha)  # Sample the topic distribution\n",
        "\n",
        "    words = []\n",
        "    for _ in range(doc_length):\n",
        "        topic = np.random.choice(\n",
        "            k, p=theta\n",
        "        )  # Sample a topic according to the topic distribution\n",
        "\n",
        "        word_idx = np.random.choice(\n",
        "            V, p=beta[topic]\n",
        "        )  # Sample a word from the chosen topic using the topic-word distribution (beta)\n",
        "        words.append(vocabulary[word_idx])\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "# Function to compare true and inferred topic-word distributions\n",
        "def compare_true_and_inferred_topics(true_beta, inferred_beta, vocabulary):\n",
        "    print(\"True Beta Matrix (Topic-Word Distribution):\")\n",
        "    for i, topic_dist in enumerate(true_beta):\n",
        "        print(\n",
        "            f\"Topic {i + 1}: \",\n",
        "            [f\"{vocabulary[j]} ({prob:.2f})\" for j, prob in enumerate(topic_dist)],\n",
        "        )\n",
        "\n",
        "    print(\"\\nInferred Beta Matrix (Topic-Word Distribution):\")\n",
        "    for topic_id, topic_words in inferred_beta:\n",
        "        word_dict = {word: prob for word, prob in topic_words}\n",
        "        ordered_topic = [\n",
        "            f\"{vocabulary[i]} ({word_dict.get(vocabulary[i], 0):.2f})\"\n",
        "            for i in range(len(vocabulary))\n",
        "        ]\n",
        "        print(f\"Topic {topic_id + 1}: {', '.join(ordered_topic)}\")\n",
        "\n",
        "\n",
        "# Test\n",
        "def test():\n",
        "    \"\"\"Test the LDA generator.\"\"\"\n",
        "    vocabulary = [\"bass\", \"pike\", \"deep\", \"tuba\", \"horn\", \"catapult\"]\n",
        "    beta = np.array(\n",
        "        [\n",
        "            [0.4, 0.4, 0.2, 0.0, 0.0, 0.0],\n",
        "            [0.0, 0.3, 0.1, 0.0, 0.3, 0.3],\n",
        "            [0.3, 0.0, 0.2, 0.3, 0.2, 0.0],\n",
        "        ]\n",
        "    )\n",
        "    alpha = np.array([0.2, 0.2, 0.2])\n",
        "    xi = 50\n",
        "\n",
        "    # Generate random documents = 100\n",
        "    documents = [lda_gen(vocabulary, alpha, beta, xi) for _ in range(50)]\n",
        "\n",
        "    dictionary = Dictionary(documents)  # 'Bag of words'\n",
        "    corpus = [dictionary.doc2bow(text) for text in documents]\n",
        "\n",
        "    # Train LDA model by using Gensim\n",
        "    model = LdaModel(corpus, id2word=dictionary, num_topics=3)\n",
        "\n",
        "    # Display inferred alpha (topic distribution per document)\n",
        "    print(\"Inferred alpha (topic distribution per document):\")\n",
        "    print(model.alpha)\n",
        "\n",
        "    # Display inferred topics\n",
        "    inferred_beta = model.show_topics(formatted=False)\n",
        "\n",
        "    # Compare true and inferred beta matrices\n",
        "    compare_true_and_inferred_topics(beta, inferred_beta, vocabulary)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()\n"
      ]
    }
  ]
}